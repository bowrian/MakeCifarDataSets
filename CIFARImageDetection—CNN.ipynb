{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow Version: %s\" % tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    \"\"\"\n",
    "    加载单批量的数据\n",
    "    \n",
    "    参数：\n",
    "    cifar10_dataset_folder_path: 数据存储目录\n",
    "    batch_id: 指定batch的编号\n",
    "    \"\"\"\n",
    "    with open(cifar10_dataset_folder_path + '/train' , mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "    \n",
    "    # features and labels\n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:/Users/pengl/Desktop/baidumatch/datasets/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-17d45c0f3224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcifar10_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/pengl/Desktop/baidumatch/datasets'\u001b[0m \u001b[1;31m# 本地路径\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;31m# 共有5个batch的训练数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_cfar10_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar10_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;31m#for i in range(2, 6):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_cfar10_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar10_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-d64c5e4f5ebb>\u001b[0m in \u001b[0;36mload_cfar10_batch\u001b[0;34m(cifar10_dataset_folder_path, batch_id)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_id\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m指定batch的编号\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar10_dataset_folder_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/train'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/pengl/Desktop/baidumatch/datasets/train'"
     ]
    }
   ],
   "source": [
    "# 加载所有训练数据\n",
    "cifar10_path = 'C:/Users/pengl/Desktop/baidumatch/datasets' # 本地路径\n",
    "# 共有5个batch的训练数据\n",
    "x_train, y_train = load_cfar10_batch(cifar10_path, 1)\n",
    "#for i in range(2, 6):\n",
    "features, labels = load_cfar10_batch(cifar10_path, 1)\n",
    "x_train, y_train = np.concatenate([x_train, features]), np.concatenate([y_train, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:/Users/pengl/Desktop/baidumatch/datasets/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3b94db234dfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# 加载测试数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar10_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/pengl/Desktop/baidumatch/datasets/test'"
     ]
    }
   ],
   "source": [
    "# 加载测试数据\n",
    "with open(cifar10_path + '/test', mode='rb') as file:\n",
    "    batch = pickle.load(file, encoding='latin1')\n",
    "    x_test = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    y_test = batch['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图片概要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 显示图片\n",
    "fig, axes = plt.subplots(nrows=3, ncols=20, sharex=True, sharey=True, figsize=(80,12))\n",
    "imgs = x_train[:60]\n",
    "\n",
    "for image, row in zip([imgs[:20], imgs[20:40], imgs[40:60]], axes):\n",
    "    for img, ax in zip(image, row):\n",
    "        ax.imshow(img)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  数据预处理¶\n",
    "输入数据的处理\n",
    "标签处理\n",
    "#### 输入数据处理\n",
    "对输入数据进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "# 重塑\n",
    "x_train_rows = x_train.reshape(x_train.shape[0], 32 * 32 * 3)\n",
    "x_test_rows = x_test.reshape(x_test.shape[0], 32 * 32 * 3)\n",
    "\n",
    "# 归一化\n",
    "x_train = minmax.fit_transform(x_train_rows)\n",
    "x_test = minmax.fit_transform(x_test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 重新变为32 x 32 x 3\n",
    "x_train = x_train.reshape(x_train.shape[0], 32, 32, 3)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32, 32, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标变量处理\n",
    "对目标变量进行one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "n_class = 10 #总共10类\n",
    "lb = LabelBinarizer().fit(np.array(range(n_class)))\n",
    "\n",
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分train与val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ratio = 0.8\n",
    "x_train_, x_val, y_train_, y_val = train_test_split(x_train, \n",
    "                                                    y_train, \n",
    "                                                    train_size=train_ratio,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建网络\n",
    "参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_shape = x_train.shape\n",
    "keep_prob = 0.6\n",
    "epochs=5\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入与标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.float32, [None, 32, 32, 3], name='inputs_')\n",
    "targets_ = tf.placeholder(tf.float32, [None, n_class], name='targets_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第一层卷积加池化\n",
    "# 32 x 32 x 3 to 32 x 32 x 64\n",
    "conv1 = tf.layers.conv2d(inputs_, 64, (2,2), padding='same', activation=tf.nn.relu, \n",
    "                         kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "# 32 x 32 x 64 to 16 x 16 x 64\n",
    "conv1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "\n",
    "# 第二层卷积加池化\n",
    "# 16 x 16 x 64 to 16 x 16 x 128\n",
    "conv2 = tf.layers.conv2d(conv1, 128, (4,4), padding='same', activation=tf.nn.relu,\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "# 16 x 16 x 128 to 8 x 8 x 128\n",
    "conv2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "\n",
    "# 重塑输出\n",
    "shape = np.prod(conv2.get_shape().as_list()[1:])\n",
    "conv2 = tf.reshape(conv2,[-1, shape])\n",
    "\n",
    "# 第一层全连接层\n",
    "# 8 x 8 x 128 to 1 x 1024\n",
    "fc1 = tf.contrib.layers.fully_connected(conv2, 1024, activation_fn=tf.nn.relu)\n",
    "fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "# 第二层全连接层\n",
    "# 1 x 1024 to 1 x 512\n",
    "fc2 = tf.contrib.layers.fully_connected(fc1, 512, activation_fn=tf.nn.relu)\n",
    "\n",
    "# logits层\n",
    "# 1 x 512 to 1 x 10\n",
    "logits_ = tf.contrib.layers.fully_connected(fc2, 10, activation_fn=None)\n",
    "logits_ = tf.identity(logits_, name='logits_')\n",
    "\n",
    "# cost & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_, labels=targets_))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "# accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits_, 1), tf.argmax(targets_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model_path='./test_cifar'\n",
    "count = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_i in range(img_shape[0]//batch_size-1):\n",
    "            feature_batch = x_train_[batch_i * batch_size: (batch_i+1)*batch_size]\n",
    "            label_batch = y_train_[batch_i * batch_size: (batch_i+1)*batch_size]\n",
    "            train_loss, _ = sess.run([cost, optimizer],\n",
    "                                     feed_dict={inputs_: feature_batch,\n",
    "                                                targets_: label_batch})\n",
    "\n",
    "            val_acc = sess.run(accuracy,\n",
    "                               feed_dict={inputs_: x_val,\n",
    "                                          targets_: y_val})\n",
    "            \n",
    "            if(count%10==0):\n",
    "                print('Epoch {:>2}, Train Loss {:.4f}, Validation Accuracy {:4f} '.format(epoch + 1, train_loss, val_acc))\n",
    "            \n",
    "            count += 1\n",
    "    \n",
    "    # 存储参数\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "test_batch_size= 100\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # 加载模型\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # 加载tensor\n",
    "    loaded_x = loaded_graph.get_tensor_by_name('inputs_:0')\n",
    "    loaded_y = loaded_graph.get_tensor_by_name('targets_:0')\n",
    "    loaded_logits = loaded_graph.get_tensor_by_name('logits_:0')\n",
    "    loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "\n",
    "    # 计算test的准确率\n",
    "    test_batch_acc_total = 0\n",
    "    test_batch_count = 0\n",
    "    \n",
    "    print(\"Begin test...\")\n",
    "    for batch_i in range(x_test.shape[0]//test_batch_size-1):\n",
    "        test_feature_batch = x_test[batch_i * test_batch_size: (batch_i+1)*test_batch_size]\n",
    "        test_label_batch = y_test[batch_i * test_batch_size: (batch_i+1)*test_batch_size]\n",
    "        test_batch_acc_total += sess.run(\n",
    "            loaded_acc,\n",
    "            feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch})\n",
    "        test_batch_count += 1\n",
    "\n",
    "    print('Test Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
